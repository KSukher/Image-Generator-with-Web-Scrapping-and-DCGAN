{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapper\n",
    "\n",
    "Here we will be generating a dataset of images to later use in the DCGAN model for testing/training and generating new images based on the inputs.\n",
    "\n",
    "For this will we use `selenium`, a browsing automation tool for controlling open-source webpages. As well as `beautifulsoup`, a Python package that parses HTML and XML documents and extracts specified data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Importing Packages\n",
    "\n",
    "Installing `chromedriver`, a standalone server that implements an open source tool called `webdriver` for the Chrome browser. As well as installing `selenium`, `beautifulsoup` and `lxml` for handling the HTML pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.0.1)\n",
      "Requirement already satisfied: selenium in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (4.1.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "conda 4.10.3 requires ruamel_yaml_conda>=0.11.14, which is not installed.\n",
      "requests 2.24.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.7 which is incompatible.\n",
      "huggingface-hub 0.1.0 requires packaging>=20.9, but you'll have packaging 20.4 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting urllib3[secure]~=1.26\n",
      "  Using cached urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from selenium) (0.19.0)\n",
      "Requirement already satisfied: idna>=2.0.0; extra == \"secure\" in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (2.10)\n",
      "Requirement already satisfied: certifi; extra == \"secure\" in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14; extra == \"secure\" in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (19.1.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4; extra == \"secure\" in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (3.1.1)\n",
      "Requirement already satisfied: async-generator>=1.10 in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.10)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (20.3.0)\n",
      "Requirement already satisfied: cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\" in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.3)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.2.2)\n",
      "Requirement already satisfied: outcome in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14; extra == \"secure\"->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.12.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\"->trio~=0.17->selenium) (2.20)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.11\n",
      "    Uninstalling urllib3-1.25.11:\n",
      "      Successfully uninstalled urllib3-1.25.11\n",
      "Successfully installed urllib3-1.26.7\n",
      "Requirement already satisfied: lxml in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (4.6.1)\n",
      "Requirement already satisfied: webdriver_manager in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: requests in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from webdriver_manager) (2.24.0)\n",
      "Requirement already satisfied: configparser in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from webdriver_manager) (5.2.0)\n",
      "Requirement already satisfied: crayons in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from webdriver_manager) (0.4.0)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ksukh\\anaconda3\\lib\\site-packages (from crayons->webdriver_manager) (0.4.4)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.7\n",
      "    Uninstalling urllib3-1.26.7:\n",
      "      Successfully uninstalled urllib3-1.26.7\n",
      "Successfully installed urllib3-1.25.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "conda 4.10.3 requires ruamel_yaml_conda>=0.11.14, which is not installed.\n",
      "selenium 4.1.0 requires urllib3[secure]~=1.26, but you'll have urllib3 1.25.11 which is incompatible.\n",
      "huggingface-hub 0.1.0 requires packaging>=20.9, but you'll have packaging 20.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install selenium\n",
    "!pip install lxml\n",
    "!pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required packages and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping Function\n",
    "\n",
    "Implementing the function for the inputted variables to generate the specific output dataset. Here `chromedriver` is setup along with the url for **shutterstock.com** which is where the images will be scrapped from.\n",
    "\n",
    "The function scrolls to the bottom and extends the page until it can't anymore and adds all the images onto a list. It continues to parse and perform the same routine through all the inputted pages. Onces all pages have been scrapped, all the images on the list are transferred to the inputted directory folder/path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapper():\n",
    "    try:\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n",
    "        driver.maximize_window()\n",
    "        \n",
    "        for i in range(1, page_num + 1):\n",
    "            url = \"https://www.shutterstock.com/search?searchterm=\" + search + \"&sort=popular&image_type=\" + image_var + \"&search_source=base_landing_page&language=en&page=\" + str(i)\n",
    "            driver.get(url)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "            time.sleep(4)\n",
    "            \n",
    "            driver_data = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "            print(\"Page \" + str(i) + \" is being scrapped...\")\n",
    "            \n",
    "            scraper = BeautifulSoup(driver_data, \"lxml\")\n",
    "            img_list = scraper.find_all(\"img\", {\"class\":\"z_h_9d80b z_h_2f2f0\"})\n",
    "            \n",
    "            for j in range(0, len(img_list)-1):\n",
    "                img_act = img_list[j].get(\"src\")\n",
    "                name = img_act.rsplit(\"/\", 1)[-1]\n",
    "                \n",
    "                try:\n",
    "                    urlretrieve(img_act, os.path.join(dataset_path, os.path.basename(img_act)))\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "        driver.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Declarations and Main Run\n",
    "\n",
    "Here is where the user can adjust the searching variables for what kind of dataset they want to use. The variables are as follow,\n",
    "\n",
    "`dataset_path` - The directory for where the output images will be stored.\n",
    "\n",
    "`search` - The term for which to search images of. (Ex. \"forest\", \"landscape\", \"portrait\", etc.)\n",
    "\n",
    "`image_var` - The type of images to be scrapped. (Ex. \"all\", \"photo\", etc.)\n",
    "\n",
    "`page_num` - Number of pages to scrape. (NOTE: The number of images scrapped from each page vary. Based on testing results, ~100 images are scrapped per page.)\n",
    "\n",
    "After the variables are set, the scrapping function will run and iterate when completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 96.0.4664\n",
      "Get LATEST chromedriver version for 96.0.4664 google-chrome\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/96.0.4664.45/chromedriver_win32.zip\n",
      "Driver has been saved in cache [C:\\Users\\ksukh\\.wdm\\drivers\\chromedriver\\win32\\96.0.4664.45]\n",
      "<ipython-input-3-39f80e898edf>:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 is being scrapped...\n",
      "Page 2 is being scrapped...\n",
      "Page 3 is being scrapped...\n",
      "Page 4 is being scrapped...\n",
      "Page 5 is being scrapped...\n",
      "Page 6 is being scrapped...\n",
      "Page 7 is being scrapped...\n",
      "Page 8 is being scrapped...\n",
      "Page 9 is being scrapped...\n",
      "Page 10 is being scrapped...\n",
      "Page 11 is being scrapped...\n",
      "Page 12 is being scrapped...\n",
      "Page 13 is being scrapped...\n",
      "Page 14 is being scrapped...\n",
      "Page 15 is being scrapped...\n",
      "Page 16 is being scrapped...\n",
      "Page 17 is being scrapped...\n",
      "Page 18 is being scrapped...\n",
      "Page 19 is being scrapped...\n",
      "Page 20 is being scrapped...\n",
      "Page 21 is being scrapped...\n",
      "Page 22 is being scrapped...\n",
      "Page 23 is being scrapped...\n",
      "Page 24 is being scrapped...\n",
      "Page 25 is being scrapped...\n",
      "Page 26 is being scrapped...\n",
      "Page 27 is being scrapped...\n",
      "Page 28 is being scrapped...\n",
      "Page 29 is being scrapped...\n",
      "Page 30 is being scrapped...\n",
      "Page 31 is being scrapped...\n",
      "Page 32 is being scrapped...\n",
      "Page 33 is being scrapped...\n",
      "Page 34 is being scrapped...\n",
      "Page 35 is being scrapped...\n",
      "Page 36 is being scrapped...\n",
      "Page 37 is being scrapped...\n",
      "Page 38 is being scrapped...\n",
      "Page 39 is being scrapped...\n",
      "Page 40 is being scrapped...\n",
      "...Scrapping complete.\n",
      "Number of images scrapped: 4123\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"Project/output\"\n",
    "\n",
    "search = \"landscape\"\n",
    "image_var = 'photo'\n",
    "page_num = 40\n",
    "\n",
    "scrapper()\n",
    "\n",
    "print(\"...Scrapping complete.\")\n",
    "\n",
    "img_num = len(os.listdir(dataset_path))\n",
    "print(\"Number of images scrapped: \" + str(img_num))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

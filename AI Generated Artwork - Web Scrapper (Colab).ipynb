{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AI Generated Artwork - Web Scrapper (Colab).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMmrlIa2F3vdyGVxDZ9tl63"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Web Scrapper\n","Here we will be generating a dataset of images to later use in the DCGAN model for testing/training and generating new images based on the inputs.\n","\n","For this will we use `selenium`, a browsing automation tool for controlling open-source webpages. As well as `beautifulsoup`, a Python package that parses HTML and XML documents and extracts specified data."],"metadata":{"id":"vHoWvSG7BAo8"}},{"cell_type":"markdown","source":["## Mount Drive\n","Mounting Google Drive so it will be accessable in Colab when transferring over scrapped images."],"metadata":{"id":"4hTsn9kwCt22"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R1oU9Joqt0kt","executionInfo":{"status":"ok","timestamp":1639593391460,"user_tz":300,"elapsed":19900,"user":{"displayName":"Kevin Sukher","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12256291807007655265"}},"outputId":"7ab7426b-9676-4951-b942-2a9e6f67da59"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Downloading and Importing Packages\n","Installing `chromedriver`, a standalone server that implements an open source tool called `webdriver` for the Chrome browser. As well as installing `selenium`, `beautifulsoup` and `lxml` for handling the HTML pages."],"metadata":{"id":"3MuWgan5DIYh"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a3GDp9ykpxq-","executionInfo":{"status":"ok","timestamp":1639593450882,"user_tz":300,"elapsed":59435,"user":{"displayName":"Kevin Sukher","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12256291807007655265"}},"outputId":"3bfb6d49-08dc-45ac-d975-a0940cda9d8e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Get:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [73.9 kB]\n","Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Get:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n","Get:16 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [691 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,898 kB]\n","Hit:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,452 kB]\n","Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,820 kB]\n","Get:21 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,461 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,230 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [12.6 kB]\n","Get:24 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.6 kB]\n","Get:25 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [933 kB]\n","Get:26 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.9 kB]\n","Fetched 12.9 MB in 7s (1,816 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n","Suggested packages:\n","  webaccounts-chromium-extension unity-chromium-extension\n","The following NEW packages will be installed:\n","  chromium-browser chromium-browser-l10n chromium-chromedriver\n","  chromium-codecs-ffmpeg-extra\n","0 upgraded, 4 newly installed, 0 to remove and 69 not upgraded.\n","Need to get 94.0 MB of archives.\n","After this operation, 324 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 95.0.4638.69-0ubuntu0.18.04.1 [1,135 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 95.0.4638.69-0ubuntu0.18.04.1 [83.6 MB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 95.0.4638.69-0ubuntu0.18.04.1 [4,249 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 95.0.4638.69-0ubuntu0.18.04.1 [4,986 kB]\n","Fetched 94.0 MB in 10s (9,510 kB/s)\n","Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n","(Reading database ... 155222 files and directories currently installed.)\n","Preparing to unpack .../chromium-codecs-ffmpeg-extra_95.0.4638.69-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-codecs-ffmpeg-extra (95.0.4638.69-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-browser.\n","Preparing to unpack .../chromium-browser_95.0.4638.69-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-browser (95.0.4638.69-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-browser-l10n.\n","Preparing to unpack .../chromium-browser-l10n_95.0.4638.69-0ubuntu0.18.04.1_all.deb ...\n","Unpacking chromium-browser-l10n (95.0.4638.69-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-chromedriver.\n","Preparing to unpack .../chromium-chromedriver_95.0.4638.69-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-chromedriver (95.0.4638.69-0ubuntu0.18.04.1) ...\n","Setting up chromium-codecs-ffmpeg-extra (95.0.4638.69-0ubuntu0.18.04.1) ...\n","Setting up chromium-browser (95.0.4638.69-0ubuntu0.18.04.1) ...\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n","Setting up chromium-chromedriver (95.0.4638.69-0ubuntu0.18.04.1) ...\n","Setting up chromium-browser-l10n (95.0.4638.69-0ubuntu0.18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Processing triggers for mime-support (3.60ubuntu1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n","cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n","Collecting selenium\n","  Downloading selenium-4.1.0-py3-none-any.whl (958 kB)\n","\u001b[K     |████████████████████████████████| 958 kB 4.3 MB/s \n","\u001b[?25hCollecting trio~=0.17\n","  Downloading trio-0.19.0-py3-none-any.whl (356 kB)\n","\u001b[K     |████████████████████████████████| 356 kB 64.3 MB/s \n","\u001b[?25hCollecting trio-websocket~=0.9\n","  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n","Collecting urllib3[secure]~=1.26\n","  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 64.2 MB/s \n","\u001b[?25hCollecting sniffio\n","  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n","Collecting async-generator>=1.9\n","  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n","Collecting outcome\n","  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.2.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n","Collecting wsproto>=0.14\n","  Downloading wsproto-1.0.0-py3-none-any.whl (24 kB)\n","Collecting cryptography>=1.3.4\n","  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n","\u001b[K     |████████████████████████████████| 3.6 MB 35.3 MB/s \n","\u001b[?25hCollecting pyOpenSSL>=0.14\n","  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n","\u001b[K     |████████████████████████████████| 55 kB 3.4 MB/s \n","\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure]~=1.26->selenium) (2021.10.8)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (2.21)\n","Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.15.0)\n","Collecting h11<1,>=0.9.0\n","  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n","\u001b[K     |████████████████████████████████| 54 kB 2.8 MB/s \n","\u001b[?25hInstalling collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.7 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed async-generator-1.10 cryptography-36.0.1 h11-0.12.0 outcome-1.1.0 pyOpenSSL-21.0.0 selenium-4.1.0 sniffio-1.2.0 trio-0.19.0 trio-websocket-0.9.2 urllib3-1.26.7 wsproto-1.0.0\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.2.6)\n","Collecting webdriver_manager\n","  Downloading webdriver_manager-3.5.2-py2.py3-none-any.whl (17 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from webdriver_manager) (2.23.0)\n","Collecting crayons\n","  Downloading crayons-0.4.0-py2.py3-none-any.whl (4.6 kB)\n","Collecting configparser\n","  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n","Collecting colorama\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver_manager) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver_manager) (2.10)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 6.1 MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver_manager) (2021.10.8)\n","Installing collected packages: urllib3, colorama, crayons, configparser, webdriver-manager\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.26.7\n","    Uninstalling urllib3-1.26.7:\n","      Successfully uninstalled urllib3-1.26.7\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","selenium 4.1.0 requires urllib3[secure]~=1.26, but you have urllib3 1.25.11 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed colorama-0.4.4 configparser-5.2.0 crayons-0.4.0 urllib3-1.25.11 webdriver-manager-3.5.2\n"]}],"source":["!apt-get update\n","!apt install chromium-chromedriver\n","!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n","!pip install beautifulsoup4\n","!pip install selenium\n","!pip install lxml\n","!pip install webdriver_manager"]},{"cell_type":"markdown","source":["Importing required packages and libraries."],"metadata":{"id":"YCKR6vTzEvIn"}},{"cell_type":"code","source":["from urllib.request import urlretrieve\n","from selenium import webdriver\n","from webdriver_manager.chrome import ChromeDriverManager\n","from selenium.webdriver.chrome.options import Options\n","from bs4 import BeautifulSoup\n","import os\n","import time"],"metadata":{"id":"tG7G316stEmC","executionInfo":{"status":"ok","timestamp":1639593451185,"user_tz":300,"elapsed":314,"user":{"displayName":"Kevin Sukher","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12256291807007655265"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Scrapping Function\n","Implementing the function for the inputted variables to generate the specific output dataset. Here `chromedriver` is setup along with the url for **shutterstock.com** which is where the images will be scrapped from.\n","\n","The function scrolls to the bottom and extends the page until it can't anymore and adds all the images onto a list. It continues to parse and perform the same routine through all the inputted pages. Onces all pages have been scrapped, all the images on the list are transferred to the inputted directory folder/path."],"metadata":{"id":"MZQrgxYjE4-n"}},{"cell_type":"code","source":["def scrapper():\n","    try:\n","        chrome_options = webdriver.ChromeOptions()\n","        chrome_options.add_argument('--headless')\n","        chrome_options.add_argument(\"--incognito\")\n","        chrome_options.add_argument('--no-sandbox')\n","        chrome_options.add_argument('--disable-dev-shm-usage') \n","        driver = webdriver.Chrome('/usr/lib/chromium-browser/chromedriver',options=chrome_options)\n","        driver.maximize_window()\n","\n","        for i in range(1, page_num + 1):\n","            url = \"https://www.shutterstock.com/search?searchterm=\" + search + \"&sort=popular&image_type=\" + image_var + \"&search_source=base_landing_page&language=en&page=\" + str(i)\n","            driver.get(url)\n","            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n","            time.sleep(4)\n","\n","            driver_data = driver.execute_script(\"return document.documentElement.outerHTML\")\n","            print(\"Page \" + str(i) + \" is being scrapped...\")\n","\n","            scraper = BeautifulSoup(driver_data, \"lxml\")\n","            img_list = scraper.find_all(\"img\", {\"class\":\"z_h_9d80b z_h_2f2f0\"})\n","\n","            for j in range(0, len(img_list)-1):\n","                img_act = img_list[j].get(\"src\")\n","                name = img_act.rsplit(\"/\", 1)[-1]\n","\n","                try:\n","                    urlretrieve(img_act, os.path.join(dataset_path, os.path.basename(img_act)))\n","\n","                except Exception as e:\n","                    print(e)\n","        driver.close()\n","\n","    except Exception as e:\n","        print(e)"],"metadata":{"id":"50s6qjXKuYGL","executionInfo":{"status":"ok","timestamp":1639593451187,"user_tz":300,"elapsed":9,"user":{"displayName":"Kevin Sukher","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12256291807007655265"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Variable Declarations and Main Run\n","Here is where the user can adjust the searching variables for what kind of dataset they want to use. The variables are as follow,\n","\n","`dataset_path` - The directory for where the output images will be stored.\n","\n","`search` - The term for which to search images of. (Ex. \"forest\", \"landscape\", \"portrait\", etc.)\n","\n","`image_var` - The type of images to be scrapped. (Ex. \"all\", \"photo\", etc.)\n","\n","`page_num` - Number of pages to scrape. (NOTE: The number of images scrapped from each page vary. Based on testing results, ~19 images are scrapped per page.)\n","\n","After the variables are set, the scrapping function will run and iterate when completed."],"metadata":{"id":"ThNhequkHbLG"}},{"cell_type":"code","source":["dataset_path = \"drive/MyDrive/Colab Notebooks/dataset/output\"\n","\n","search = \"landscape\"\n","image_var = 'photo'\n","page_num = 20\n","\n","scrapper()\n","\n","print(\"...Scrapping complete.\")\n","\n","img_num = len(os.listdir(dataset_path))\n","print(\"Number of images scrapped: \" + str(img_num))"],"metadata":{"id":"_fLBO4ZEuh5w","executionInfo":{"status":"ok","timestamp":1639594033191,"user_tz":300,"elapsed":582011,"user":{"displayName":"Kevin Sukher","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12256291807007655265"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4d97d476-f27e-424b-fe6d-8c2e30bf3de4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n","  \n"]},{"output_type":"stream","name":"stdout","text":["Page 1 is being scrapped...\n","Page 2 is being scrapped...\n","Page 3 is being scrapped...\n","Page 4 is being scrapped...\n","Page 5 is being scrapped...\n","Page 6 is being scrapped...\n","Page 7 is being scrapped...\n","Page 8 is being scrapped...\n","Page 9 is being scrapped...\n","Page 10 is being scrapped...\n","Page 11 is being scrapped...\n","Page 12 is being scrapped...\n","Page 13 is being scrapped...\n","Page 14 is being scrapped...\n","Page 15 is being scrapped...\n","Page 16 is being scrapped...\n","Page 17 is being scrapped...\n","Page 18 is being scrapped...\n","Page 19 is being scrapped...\n","Page 20 is being scrapped...\n","...Scrapping complete.\n","Number of images scrapped: 380\n"]}]}]}